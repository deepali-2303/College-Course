{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2111a626",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTree(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m dt\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X, y, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     58\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_feature_index\n\u001b[0;32m     59\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_threshold\n\u001b[1;32m---> 60\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subtree\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     58\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_feature_index\n\u001b[0;32m     59\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_threshold\n\u001b[1;32m---> 60\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subtree\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     58\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_feature_index\n\u001b[0;32m     59\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_threshold\n\u001b[1;32m---> 60\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_left, y_left, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m subtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X_right, y_right, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subtree\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, depth):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mbincount(y))  \u001b[38;5;66;03m# Return the most common class label\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     best_feature_index, best_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_best_split(X, y)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_feature_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Provided dataset\n",
    "data = \"\"\"Feature_1,Feature_2,Feature_3,Feature_4,Feature_5,Pass_Quality_Control\n",
    "1.2,3.4,5.6,7.8,9.0,0\n",
    "2.3,4.5,6.7,8.9,10.1,1\n",
    "3.4,5.6,7.8,9.0,11.2,0\n",
    "4.5,6.7,8.9,10.1,12.3,0\n",
    "5.6,7.8,9.0,11.2,13.4,1\n",
    "6.7,8.9,10.1,12.3,14.5,1\n",
    "7.8,9.0,11.2,13.4,15.6,0\n",
    "8.9,10.1,12.3,14.5,16.7,1\n",
    "9.0,11.2,13.4,15.6,17.8,0\n",
    "10.1,12.3,14.5,16.7,18.9,1\"\"\"\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = np.genfromtxt(data.splitlines(), delimiter=',', skip_header=1)\n",
    "\n",
    "# Split features (X) and labels (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "# X_test is your test data\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dt = DecisionTree(max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Print the tree structure\n",
    "# dt.print_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b60ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        base_entropy = self._entropy(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                n_total = len(y_left) + len(y_right)\n",
    "                left_weight = len(y_left) / n_total\n",
    "                right_weight = len(y_right) / n_total\n",
    "                gain = base_entropy - (left_weight * self._entropy(y_left) +\n",
    "                                       right_weight * self._entropy(y_right))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return np.argmax(np.bincount(y))  # Return the most common class label\n",
    "\n",
    "        best_feature_index, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        if best_feature_index is None:\n",
    "            return np.argmax(np.bincount(y))  # Return the most common class label\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split_dataset(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        subtree = {}\n",
    "        subtree['feature_index'] = best_feature_index\n",
    "        subtree['threshold'] = best_threshold\n",
    "        subtree['left'] = self._build_tree(X_left, y_left, depth + 1)\n",
    "        subtree['right'] = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _print_tree(self, tree, depth=0, indent=\"  \"):\n",
    "        if isinstance(tree, int):\n",
    "            print(indent * depth + \"Predict:\", tree)\n",
    "            return\n",
    "\n",
    "        print(indent * depth + f\"Feature {tree['feature_index']} <= {tree['threshold']}\")\n",
    "        print(indent * (depth + 1) + \"--> True:\")\n",
    "        self._print_tree(tree['left'], depth + 1, indent)\n",
    "        print(indent * (depth + 1) + \"--> False:\")\n",
    "        self._print_tree(tree['right'], depth + 1, indent)\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"Decision Tree:\")\n",
    "        self._print_tree(self.tree_)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if isinstance(tree, int):\n",
    "            return tree\n",
    "\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_sample(x, self.tree_))\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0ce675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Feature 0 <= 4.5\n",
      "  --> True:\n",
      "  Feature 0 <= 2.3\n",
      "    --> True:\n",
      "    Feature 0 <= 1.2\n",
      "      --> True:\n",
      "      Predict: 0\n",
      "      --> False:\n",
      "      Predict: 1\n",
      "    --> False:\n",
      "    Predict: 0\n",
      "  --> False:\n",
      "  Feature 0 <= 6.7\n",
      "    --> True:\n",
      "    Predict: 1\n",
      "    --> False:\n",
      "    Feature 0 <= 7.8\n",
      "      --> True:\n",
      "      Predict: 0\n",
      "      --> False:\n",
      "      Feature 0 <= 8.9\n",
      "        --> True:\n",
      "        Predict: 1\n",
      "        --> False:\n",
      "        Feature 0 <= 9.0\n",
      "          --> True:\n",
      "          Predict: 0\n",
      "          --> False:\n",
      "          Predict: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        base_entropy = self._entropy(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                n_total = len(y_left) + len(y_right)\n",
    "                left_weight = len(y_left) / n_total\n",
    "                right_weight = len(y_right) / n_total\n",
    "                gain = base_entropy - (left_weight * self._entropy(y_left) +\n",
    "                                       right_weight * self._entropy(y_right))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return int(np.argmax(np.bincount(y.astype(int))))  # Convert y to integers\n",
    "            \n",
    "        best_feature_index, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        if best_feature_index is None:\n",
    "            return int(np.argmax(np.bincount(y.astype(int))))  # Convert y to integers\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split_dataset(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        subtree = {}\n",
    "        subtree['feature_index'] = best_feature_index\n",
    "        subtree['threshold'] = best_threshold\n",
    "        subtree['left'] = self._build_tree(X_left, y_left, depth + 1)\n",
    "        subtree['right'] = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _print_tree(self, tree, depth=0, indent=\"  \"):\n",
    "        if isinstance(tree, int):\n",
    "            print(indent * depth + \"Predict:\", tree)\n",
    "            return\n",
    "\n",
    "        print(indent * depth + f\"Feature {tree['feature_index']} <= {tree['threshold']}\")\n",
    "        print(indent * (depth + 1) + \"--> True:\")\n",
    "        self._print_tree(tree['left'], depth + 1, indent)\n",
    "        print(indent * (depth + 1) + \"--> False:\")\n",
    "        self._print_tree(tree['right'], depth + 1, indent)\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"Decision Tree:\")\n",
    "        self._print_tree(self.tree_)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if isinstance(tree, int):\n",
    "            return tree\n",
    "\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_sample(x, self.tree_))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Provided dataset\n",
    "data = \"\"\"Feature_1,Feature_2,Feature_3,Feature_4,Feature_5,Pass_Quality_Control\n",
    "1.2,3.4,5.6,7.8,9.0,0\n",
    "2.3,4.5,6.7,8.9,10.1,1\n",
    "3.4,5.6,7.8,9.0,11.2,0\n",
    "4.5,6.7,8.9,10.1,12.3,0\n",
    "5.6,7.8,9.0,11.2,13.4,1\n",
    "6.7,8.9,10.1,12.3,14.5,1\n",
    "7.8,9.0,11.2,13.4,15.6,0\n",
    "8.9,10.1,12.3,14.5,16.7,1\n",
    "9.0,11.2,13.4,15.6,17.8,0\n",
    "10.1,12.3,14.5,16.7,18.9,1\"\"\"\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = np.genfromtxt(data.splitlines(), delimiter=',', skip_header=1)\n",
    "\n",
    "# Split features (X) and labels (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dt = DecisionTree(max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Print the tree structure\n",
    "dt.print_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "add0d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node at depth 0:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 0.1245\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 4.5000\n",
      "Node at depth 1:\n",
      "  - Base Entropy: 0.8113\n",
      "  - Best Gain: 0.3113\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 2.3000\n",
      "Node at depth 2:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 1.0000\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 1.2000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Leaf Node: Predicted Class: 1\n",
      "Leaf Node: Predicted Class: 0\n",
      "Node at depth 1:\n",
      "  - Base Entropy: 0.9183\n",
      "  - Best Gain: 0.2516\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 6.7000\n",
      "Leaf Node: Predicted Class: 1\n",
      "Node at depth 2:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 0.3113\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 7.8000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Node at depth 3:\n",
      "  - Base Entropy: 0.9183\n",
      "  - Best Gain: 0.2516\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 8.9000\n",
      "Leaf Node: Predicted Class: 1\n",
      "Node at depth 4:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 1.0000\n",
      "  - Best Feature Index: 0\n",
      "  - Best Threshold: 9.0000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Leaf Node: Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        base_entropy = self._entropy(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                n_total = len(y_left) + len(y_right)\n",
    "                left_weight = len(y_left) / n_total\n",
    "                right_weight = len(y_right) / n_total\n",
    "                gain = base_entropy - (left_weight * self._entropy(y_left) +\n",
    "                                       right_weight * self._entropy(y_right))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold, best_gain, base_entropy\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            most_common_label = np.argmax(np.bincount(y.astype(int)))  # Convert y to integers\n",
    "            print(f\"Leaf Node: Predicted Class: {most_common_label}\")\n",
    "            return most_common_label\n",
    "            \n",
    "        best_feature_index, best_threshold, best_gain, base_entropy = self._find_best_split(X, y)\n",
    "        \n",
    "        print(f\"Node at depth {depth}:\")\n",
    "        print(f\"  - Base Entropy: {base_entropy:.4f}\")\n",
    "        print(f\"  - Best Gain: {best_gain:.4f}\")\n",
    "        print(f\"  - Best Feature Index: {best_feature_index}\")\n",
    "        print(f\"  - Best Threshold: {best_threshold:.4f}\")\n",
    "\n",
    "        if best_feature_index is None:\n",
    "            most_common_label = np.argmax(np.bincount(y.astype(int)))  # Convert y to integers\n",
    "            print(f\"  - Leaf Node: Predicted Class: {most_common_label}\")\n",
    "            return most_common_label\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split_dataset(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        subtree = {}\n",
    "        subtree['feature_index'] = best_feature_index\n",
    "        subtree['threshold'] = best_threshold\n",
    "        subtree['left'] = self._build_tree(X_left, y_left, depth + 1)\n",
    "        subtree['right'] = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _print_tree(self, tree, depth=0, indent=\"  \"):\n",
    "        if isinstance(tree, int):\n",
    "            print(indent * depth + \"Predict:\", tree)\n",
    "            return\n",
    "\n",
    "        print(indent * depth + f\"Feature {tree['feature_index']} <= {tree['threshold']}\")\n",
    "        print(indent * (depth + 1) + \"--> True:\")\n",
    "        self._print_tree(tree['left'], depth + 1, indent)\n",
    "        print(indent * (depth + 1) + \"--> False:\")\n",
    "        self._print_tree(tree['right'], depth + 1, indent)\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"Decision Tree:\")\n",
    "        self._print_tree(self.tree_)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if isinstance(tree, int):\n",
    "            return tree\n",
    "\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_sample(x, self.tree_))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Provided dataset\n",
    "data = \"\"\"Feature_1,Feature_2,Feature_3,Feature_4,Feature_5,Pass_Quality_Control\n",
    "1.2,3.4,5.6,7.8,9.0,0\n",
    "2.3,4.5,6.7,8.9,10.1,1\n",
    "3.4,5.6,7.8,9.0,11.2,0\n",
    "4.5,6.7,8.9,10.1,12.3,0\n",
    "5.6,7.8,9.0,11.2,13.4,1\n",
    "6.7,8.9,10.1,12.3,14.5,1\n",
    "7.8,9.0,11.2,13.4,15.6,0\n",
    "8.9,10.1,12.3,14.5,16.7,1\n",
    "9.0,11.2,13.4,15.6,17.8,0\n",
    "10.1,12.3,14.5,16.7,18.9,1\"\"\"\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = np.genfromtxt(data.splitlines(), delimiter=',', skip_header=1)\n",
    "\n",
    "# Split features (X) and labels (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dt = DecisionTree(max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Print the tree structure\n",
    "# dt.print_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cecfd6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node at depth 0:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 0.1245\n",
      "  - Best Feature: Feature_1 <= 4.5000\n",
      "Node at depth 1:\n",
      "  - Base Entropy: 0.8113\n",
      "  - Best Gain: 0.3113\n",
      "  - Best Feature: Feature_1 <= 2.3000\n",
      "Node at depth 2:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 1.0000\n",
      "  - Best Feature: Feature_1 <= 1.2000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Leaf Node: Predicted Class: 1\n",
      "Leaf Node: Predicted Class: 0\n",
      "Node at depth 1:\n",
      "  - Base Entropy: 0.9183\n",
      "  - Best Gain: 0.2516\n",
      "  - Best Feature: Feature_1 <= 6.7000\n",
      "Leaf Node: Predicted Class: 1\n",
      "Node at depth 2:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 0.3113\n",
      "  - Best Feature: Feature_1 <= 7.8000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Node at depth 3:\n",
      "  - Base Entropy: 0.9183\n",
      "  - Best Gain: 0.2516\n",
      "  - Best Feature: Feature_1 <= 8.9000\n",
      "Leaf Node: Predicted Class: 1\n",
      "Node at depth 4:\n",
      "  - Base Entropy: 1.0000\n",
      "  - Best Gain: 1.0000\n",
      "  - Best Feature: Feature_1 <= 9.0000\n",
      "Leaf Node: Predicted Class: 0\n",
      "Leaf Node: Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_names = None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = -np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        base_entropy = self._entropy(y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                n_total = len(y_left) + len(y_right)\n",
    "                left_weight = len(y_left) / n_total\n",
    "                right_weight = len(y_right) / n_total\n",
    "                gain = base_entropy - (left_weight * self._entropy(y_left) +\n",
    "                                       right_weight * self._entropy(y_right))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if self.feature_names is not None:\n",
    "            best_feature_name = self.feature_names[best_feature_index]\n",
    "        else:\n",
    "            best_feature_name = f\"Feature_{best_feature_index}\"\n",
    "            \n",
    "        return best_feature_index, best_threshold, best_gain, base_entropy, best_feature_name\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            most_common_label = np.argmax(np.bincount(y.astype(int)))  # Convert y to integers\n",
    "            print(f\"Leaf Node: Predicted Class: {most_common_label}\")\n",
    "            return most_common_label\n",
    "            \n",
    "        best_feature_index, best_threshold, best_gain, base_entropy, best_feature_name = self._find_best_split(X, y)\n",
    "        \n",
    "        print(f\"Node at depth {depth}:\")\n",
    "        print(f\"  - Base Entropy: {base_entropy:.4f}\")\n",
    "        print(f\"  - Best Gain: {best_gain:.4f}\")\n",
    "        print(f\"  - Best Feature: {best_feature_name} <= {best_threshold:.4f}\")\n",
    "\n",
    "        if best_feature_index is None:\n",
    "            most_common_label = np.argmax(np.bincount(y.astype(int)))  # Convert y to integers\n",
    "            print(f\"  - Leaf Node: Predicted Class: {most_common_label}\")\n",
    "            return most_common_label\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self._split_dataset(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        subtree = {}\n",
    "        subtree['feature_index'] = best_feature_index\n",
    "        subtree['threshold'] = best_threshold\n",
    "        subtree['left'] = self._build_tree(X_left, y_left, depth + 1)\n",
    "        subtree['right'] = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return subtree\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        self.feature_names = feature_names\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _print_tree(self, tree, depth=0, indent=\"  \"):\n",
    "        if isinstance(tree, int):\n",
    "            print(indent * depth + \"Predict:\", tree)\n",
    "            return\n",
    "\n",
    "        print(indent * depth + f\"{self.feature_names[tree['feature_index']]} <= {tree['threshold']}\")\n",
    "        print(indent * (depth + 1) + \"--> True:\")\n",
    "        self._print_tree(tree['left'], depth + 1, indent)\n",
    "        print(indent * (depth + 1) + \"--> False:\")\n",
    "        self._print_tree(tree['right'], depth + 1, indent)\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"Decision Tree:\")\n",
    "        self._print_tree(self.tree_)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if isinstance(tree, int):\n",
    "            return tree\n",
    "\n",
    "        feature_index = tree['feature_index']\n",
    "        threshold = tree['threshold']\n",
    "\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self._predict_sample(x, self.tree_))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Provided dataset\n",
    "data = \"\"\"Feature_1,Feature_2,Feature_3,Feature_4,Feature_5,Pass_Quality_Control\n",
    "1.2,3.4,5.6,7.8,9.0,0\n",
    "2.3,4.5,6.7,8.9,10.1,1\n",
    "3.4,5.6,7.8,9.0,11.2,0\n",
    "4.5,6.7,8.9,10.1,12.3,0\n",
    "5.6,7.8,9.0,11.2,13.4,1\n",
    "6.7,8.9,10.1,12.3,14.5,1\n",
    "7.8,9.0,11.2,13.4,15.6,0\n",
    "8.9,10.1,12.3,14.5,16.7,1\n",
    "9.0,11.2,13.4,15.6,17.8,0\n",
    "10.1,12.3,14.5,16.7,18.9,1\"\"\"\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = np.genfromtxt(data.splitlines(), delimiter=',', skip_header=1)\n",
    "\n",
    "# Split features (X) and labels (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Feature names\n",
    "feature_names = [\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Feature_5\"]\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dt = DecisionTree(max_depth=5)\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(X, y, feature_names=feature_names)\n",
    "\n",
    "# Print the tree structure\n",
    "# dt.print_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303cb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
